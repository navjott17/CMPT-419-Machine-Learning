{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaZNv7FK0EhO"
      },
      "source": [
        "# **CMPT 726/419 A3 Q3: Optimization Algorithms**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**The test cases for this assignment are provided here. Don't mess with them!**\n",
        "The assignment starts below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iI09ow5Z4G9w"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5W2quORiadr"
      },
      "outputs": [],
      "source": [
        "def generate_submission_hashes(optimizer, data_file, out_file):\n",
        "    \"\"\"Writes hashes of the result of running [optimizer] on data from\n",
        "    [data_file] to [out_file]. After this is run, [out_file] can be submitted to\n",
        "    Kaggle for evaluation.\n",
        "\n",
        "    Args:\n",
        "    optimizer   -- the optimizer being evaluated\n",
        "    data_file   -- a .pt file wrapping a list where each item is a dictionary\n",
        "                    as follows:\n",
        "                    {\n",
        "                        \"hparams\": dictionary of hyperparameters\n",
        "                        \"start_weights\": array of starting weights\n",
        "                        \"gradients\": list of gradients\n",
        "                        \"z\": random vector\n",
        "                    }\n",
        "    out_file    -- file to write outputs to\n",
        "    \"\"\"\n",
        "    data = torch.load(data_file)\n",
        "    results = []\n",
        "\n",
        "    for d in data:\n",
        "        weights, hparams, state_info = d[\"start_weights\"], d[\"hparams\"], {}\n",
        "        for g in d[\"gradients\"]:\n",
        "            weights, state_info = optimizer(weights, g, **state_info, **hparams)\n",
        "        results.append(np.linalg.norm(weights - d[\"z\"]))\n",
        "\n",
        "    results = [[idx, r] for idx,r in enumerate(results)]\n",
        "    with open(out_file, \"w+\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Index\", \"Hash\"])\n",
        "        for r in results:\n",
        "            writer.writerow(r)\n",
        "\n",
        "    tqdm.write(f\"Wrote solution to {out_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "noO7ERcksnpM"
      },
      "outputs": [],
      "source": [
        "def equals_(x, y, tol=1e-4):\n",
        "    \"\"\"Returns if inputs [x] and [y] are equal, with equality defined as:\n",
        "    1. [x] and [y] have identical hierarchical structure\n",
        "    2. NumPy arrays in [x] and [y] must be equal up to some tolerance\n",
        "    \"\"\"\n",
        "    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n",
        "        if not x.shape == y.shape:\n",
        "            return False\n",
        "        else:\n",
        "            cond1 = np.max(x / y) < 10 \n",
        "            cond2 = np.min(x / y) > .1\n",
        "            cond3 = np.mean(np.square(x - y)) < tol\n",
        "            return cond1 and cond2 and cond3\n",
        "    elif isinstance(x, dict) and isinstance(y, dict):\n",
        "        if not x.keys() == y.keys():\n",
        "            return False\n",
        "        else:\n",
        "            return all([equals_(x[k],y[k]) for k in x.keys()])\n",
        "    elif isinstance(x, tuple) and isinstance(y, tuple):\n",
        "        if not len(x) == len(y):\n",
        "            return False\n",
        "        else:\n",
        "            return all([equals_(a,b) for a,b in zip(x,y)])\n",
        "    elif isinstance(x, (int, float)) and isinstance(y, (int, float)):\n",
        "        return  x / y > .1 and x / y < 10 and abs(x - y) < 1e-3\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def test_optimizer(optimizer, weights, gradients, state_infos, hparams, results):\n",
        "    \"\"\"Prints test case information for [optimizer]. In the ith test, the ith\n",
        "    elements of [weights], [gradients], [state_infos], and [hparams] are input\n",
        "    to [optimizer], and the outputs of [optimizer] on these inputs are compared\n",
        "    with the ith [element] of [results].\n",
        "\n",
        "    Args:\n",
        "    optimizer   -- an optimizer function\n",
        "    weights     -- a list where each element is an array of weights\n",
        "    gradients   -- a list where each element is an array of gradients\n",
        "    state_infos -- a list where each element is a dictionary encoding the a \n",
        "                    state of the optimizer\n",
        "    hparams     -- a list list of hyperparameters\n",
        "    results     -- a list where each element is an array of updated weights\n",
        "    \"\"\"\n",
        "    for w,g,s,h,r in zip(weights, gradients, state_infos, hparams, results):\n",
        "        try:\n",
        "            new_weights, new_state_info = optimizer(w, g, **s, **h)\n",
        "            if equals_((new_weights, new_state_info), r):\n",
        "                tqdm.write(f\"=========== Test case passed :) ===========\")\n",
        "            else:\n",
        "                tqdm.write(f\"=========== Test case failed with incorrect output:( ===========\")\n",
        "                tqdm.write(f\"Input weights:\\n{w}\\n\")\n",
        "                tqdm.write(f\"Input gradients:\\n{g}\\n\")\n",
        "                tqdm.write(f\"Input state information:\\n{s}\\n\")\n",
        "                tqdm.write(f\"Input hyperparameters:{s}\\n\")\n",
        "                tqdm.write(f\"Expected new weights:{r[0]}\\n\")\n",
        "                tqdm.write(f\"Output new weights:{new_weights}\\n\")\n",
        "                tqdm.write(f\"Expected new state information:{r[1]}\\n\")\n",
        "                tqdm.write(f\"Output new state information:{new_state_info}\\n\")\n",
        "        except Exception:\n",
        "            tqdm.write(f\"=========== Test case failed with error:( ===========\")\n",
        "            tqdm.write(f\"Input weights:\\n{w}\\n\")\n",
        "            tqdm.write(f\"Input gradients:\\n{g}\\n\")\n",
        "            tqdm.write(f\"Input state information:\\n{s}\\n\")\n",
        "            tqdm.write(f\"Input hyperparameters:{s}\\n\")\n",
        "            traceback.print_exc()\n",
        "    \n",
        "    tqdm.write(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ReWVaJyMzLeP"
      },
      "outputs": [],
      "source": [
        "def unzip(input): return zip(*input)\n",
        "\n",
        "\n",
        "def test_sgd_with_momentum(optimizer):\n",
        "    \n",
        "    test_cases = [\n",
        "        [np.array([[0.13184504,-0.2324543,0.151657,-0.3217226,0.2666803,]]),\n",
        "        np.array([[-1.8109183,-1.4753062,-1.4077585,-2.0159426,-1.916701,]]),\n",
        "        {'update': 0},\n",
        "        {'lr': 1, 'mm': 0.9},\n",
        "        (np.array([[1.9427633,1.2428519,1.5594155,1.69422,2.1833813,]]),\n",
        "        {'update': np.array([[1.8109183,1.4753062,1.4077585,2.0159426,1.916701,]])})],\n",
        "\n",
        "        [np.array([[1.9427633,1.2428519,1.5594155,1.69422,2.1833813,]]),\n",
        "        np.array([[1.5052484,2.9311047,8.23906,7.965716,7.5918927,]]),\n",
        "        {'update': np.array([[1.8109183,1.4753062,1.4077585,2.0159426,1.916701,]])},\n",
        "        {'lr': 1, 'mm': 0.9},\n",
        "        (np.array([[2.0673413,-0.36047733,-5.4126625,-4.4571476,-3.6834805,]]),\n",
        "        {'update': np.array([[0.124578,-1.6033292,-6.972078,-6.1513677,-5.866862,]])})],\n",
        "\n",
        "        [np.array([[2.0673413,-0.36047733,-5.4126625,-4.4571476,-3.6834805,]]),\n",
        "        np.array([[-2.9772832,-7.9956026,-8.096151,-10.29695,-2.810056,]]),\n",
        "        {'update': np.array([[0.124578,-1.6033292,-6.972078,-6.1513677,-5.866862,]])},\n",
        "        {'lr': 1, 'mm': 0.9},\n",
        "        (np.array([[5.156745,6.192129,-3.591381,0.30357218,-6.1536,]]),\n",
        "        {'update': np.array([[3.0894034,6.5526066,1.8212814,4.76072,-2.4701197,]])})],\n",
        "\n",
        "        [np.array([[-0.2871352,-0.02784033,0.23093075,0.41601038,-0.15191871,]]),\n",
        "        np.array([[-1.9271317,-1.5067427,-0.19569726,-1.359275,-2.083497,]]),\n",
        "        {'update': 0},\n",
        "        {'lr': 2, 'mm': 0},\n",
        "        (np.array([[3.5671282,2.985645,0.6223253,3.1345603,4.015075,]]),\n",
        "        {'update': np.array([[3.8542633,3.0134854,0.39139453,2.71855,4.166994,]])})],\n",
        "\n",
        "        [np.array([[3.5671282,2.985645,0.6223253,3.1345603,4.015075,]]),\n",
        "        np.array([[11.763039,5.12741,1.0657036,7.0632687,4.160217,]]),\n",
        "        {'update': np.array([[3.8542633,3.0134854,0.39139453,2.71855,4.166994,]])},\n",
        "        {'lr': 2, 'mm': 0},\n",
        "        (np.array([[-19.95895,-7.2691746,-1.509082,-10.991977,-4.3053584,]]),\n",
        "        {'update': np.array([[-23.526077,-10.25482,-2.1314073,-14.126537,-8.320434,]])})],\n",
        "\n",
        "        [np.array([[-19.95895,-7.2691746,-1.509082,-10.991977,-4.3053584,]]),\n",
        "        np.array([[-17.727173,-46.071632,-28.641071,-26.136875,-21.374857,]]),\n",
        "        {'update': np.array([[-23.526077,-10.25482,-2.1314073,-14.126537,-8.320434,]])},\n",
        "        {'lr': 2, 'mm': 0},\n",
        "        (np.array([[15.495396,84.87409,55.77306,41.281773,38.444355,]]),\n",
        "        {'update': np.array([[35.454346,92.143265,57.282143,52.27375,42.749714,]])})],\n",
        "    ]\n",
        "    test_optimizer(optimizer, *unzip(test_cases))\n",
        "\n",
        "def test_adam(optimizer):\n",
        "    test_cases = [\n",
        "        [np.array([[0.37969318,-0.10243413,-0.39066148,-0.23623134,-0.1174912,]]),\n",
        "        np.array([[-0.25712273,-2.4049883,-0.8926169,-1.1141441,-2.6648488,]]),\n",
        "        {'t': 0, 'm': 0, 'v': 0},\n",
        "        {'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08},\n",
        "        (np.array([[0.47969317,-0.00243413,-0.29066148,-0.13623133,-0.0174912,]]),\n",
        "        {'t': 1, 'm': np.array([[-0.02571227,-0.24049883,-0.0892617,-0.11141441,-0.2664849,]]), 'v': np.array([[6.6112094e-05,5.7839686e-03,7.9676503e-04,1.2413171e-03,7.1014194e-03,]])})],\n",
        "\n",
        "        [np.array([[0.47969317,-0.00243413,-0.29066148,-0.13623135,-0.0174912,]]),\n",
        "        np.array([[-0.8544274,-1.9550852,-1.49527,-0.74472547,-2.1240926,]]),\n",
        "        {'t': 1, 'm': np.array([[-0.02571227,-0.24049883,-0.0892617,-0.11141441,-0.2664849,]]), 'v': np.array([[6.6112094e-05,5.7839686e-03,7.9676503e-04,1.2413171e-03,7.1014194e-03,]])},\n",
        "        {'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08},\n",
        "        (np.array([[0.57025313,0.09650254,-0.19242549,-0.03916606,0.08129226,]]),\n",
        "        {'t': 2, 'm': np.array([[-0.10858379,-0.41195744,-0.22986253,-0.17474551,-0.45224565,]]), 'v': np.array([[0.00079609,0.00960054,0.0030318,0.00179469,0.01160609,]])})],\n",
        "\n",
        "        [np.array([[0.57025313,0.09650254,-0.19242549,-0.03916607,0.08129227,]]),\n",
        "        np.array([[-0.2080417,-1.7005428,-0.2782645,-1.4111687,-0.94990975,]]),\n",
        "        {'t': 2, 'm': np.array([[-0.10858379,-0.41195744,-0.22986253,-0.17474551,-0.45224565,]]), 'v': np.array([[0.00079609,0.00960054,0.0030318,0.00179469,0.01160609,]])},\n",
        "        {'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08},\n",
        "        (np.array([[0.65293866,0.19428656,-0.10735527,0.05881965,0.17200929,]]),\n",
        "        {'t': 3, 'm': np.array([[-0.11852959,-0.54081595,-0.2347027,-0.29838783,-0.5020121,]]), 'v': np.array([[0.00083858,0.01248279,0.0031062,0.00378429,0.01249681,]])})],\n",
        "\n",
        "        [np.array([[-0.42728347,-0.18973799,0.23884566,0.38648924,0.22987218,]]),\n",
        "        np.array([[-2.4048717,-2.3448138,-0.23648517,-0.7440031,-0.71357477,]]),\n",
        "        {'t': 0, 'm': 0, 'v': 0},\n",
        "        {'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08},\n",
        "        (np.array([[-0.32728347,-0.08973799,0.33884567,0.48648924,0.3298722,]]),\n",
        "        {'t': 1, 'm': np.array([[-0.24048717,-0.23448138,-0.02364852,-0.07440031,-0.07135748,]]), 'v': np.array([[5.7834079e-03,5.4981522e-03,5.5925237e-05,5.5354065e-04,5.0918898e-04,]])})],\n",
        "\n",
        "        [np.array([[-0.32728347,-0.089738,0.33884564,0.48648924,0.3298722,]]),\n",
        "        np.array([[-0.34394962,-0.30774856,-0.319684,-0.51832855,-0.3830285,]]),\n",
        "        {'t': 1, 'm': np.array([[-0.24048717,-0.23448138,-0.02364852,-0.07440031,-0.07135748,]]), 'v': np.array([[5.7834079e-03,5.4981522e-03,5.5925237e-05,5.5354065e-04,5.0918898e-04,]])},\n",
        "        {'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08},\n",
        "        (np.array([[-0.25041252,-0.01361425,0.43851656,0.5840107,0.4241112,]]),\n",
        "        {'t': 2, 'm': np.array([[-0.25083342,-0.2418081,-0.05325206,-0.11879314,-0.10252458,]]), 'v': np.array([[0.00589593,0.00558736,0.00015807,0.00082165,0.00065539,]])})],\n",
        "\n",
        "        [np.array([[-0.25041252,-0.01361425,0.43851656,0.5840107,0.42411122,]]),\n",
        "        np.array([[-0.01939912,-0.8510869,-0.1728474,-0.39118072,-0.16054946,]]),\n",
        "        {'t': 2, 'm': np.array([[-0.25083342,-0.2418081,-0.05325206,-0.11879314,-0.10252458,]]), 'v': np.array([[0.00589593,0.00558736,0.00015807,0.00082165,0.00065539,]])},\n",
        "        {'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08},\n",
        "        (np.array([[-0.19048236,0.06339748,0.5346486,0.67854196,0.507998,]]),\n",
        "        {'t': 3, 'm': np.array([[-0.22768998,-0.30273598,-0.06521159,-0.14603189,-0.10832706,]]), 'v': np.array([[0.00589041,0.00630612,0.00018779,0.00097385,0.00068051,]])})],\n",
        "    ]\n",
        "    test_optimizer(optimizer, *unzip(test_cases))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHVOWWXN0whr"
      },
      "source": [
        "# Directions\n",
        "Don't edit anything above this cell.\n",
        "See the PDF. Read it carefully. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0jbh3kbY3-9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425ee9a7-62b4-4cea-f4a0-7e45d9aa2a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.core.display import update_display\n",
        "def sgd_with_momentum(weights, gradient, lr=1, mm=.9, update=0):\n",
        "    \"\"\"Performs one step of gradient descent on weights [weights] with gradient\n",
        "    [gradient] using SGD.\n",
        "\n",
        "    Args:\n",
        "    weights   (array): weights being optimized\n",
        "    gradient  (array): gradients of [weights]\n",
        "    lr        (float): the learning rate\n",
        "    mm        (float): momentum parameter\n",
        "    update    (array): the change in weights being optimized computed during the\n",
        "                        previous iteration, or zero on the first iteration\n",
        "\n",
        "    Returns:\n",
        "    A (weights, dict) tuple where [weights] is an array of weights after one\n",
        "    step of SGD, and [dict] is a dictionary with a key 'update' mapping to an\n",
        "    array that is the change in [weights] computed during the optimizer step.\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE STARTS HERE ################################################\n",
        "    update = mm * update - lr*gradient\n",
        "    weights = weights + update\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ##################################################\n",
        "    assert isinstance(weights, np.ndarray)\n",
        "    assert isinstance(update, np.ndarray)\n",
        "    assert weights.shape == update.shape  \n",
        "    return weights, {\"update\": update}\n",
        "\n",
        "test_sgd_with_momentum(sgd_with_momentum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RZhSVkk13_2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf2df32-9d79-421c-8a97-41222f61a6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "=========== Test case passed :) ===========\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def adam(weights, gradient, lr=1, t=0, m=0, v=0, beta1=.9, beta2=.999, epsilon=1e-8):\n",
        "    \"\"\"Performs one step of gradient descent on weights [weights] with gradient\n",
        "    [gradient] using ADAM.\n",
        "\n",
        "    Args:\n",
        "    weights     (array): the weights being optimized from the prior iteration\n",
        "    gradient    (array): gradient of [weights]\n",
        "    lr          (float): the learning rate\n",
        "    t           (int): index of prior iteration\n",
        "    m           (array): first moment vector from prior iteration\n",
        "    v           (array): second moment vector from prior iteration\n",
        "    beta1       (float): exponential decay rate of first moment estimate\n",
        "    beta2       (float): exponential decay rate of second moment estimate\n",
        "    epsilon     (float): small number for preventing math errors\n",
        "\n",
        "    Returns:\n",
        "    A (weights, dict) tuple where [weights] is an array of weights after one\n",
        "    step of SGD, and [dict] is a dictionary as follows:\n",
        "\n",
        "    {\n",
        "        \"t\": the index of the just-run iteration\n",
        "        \"m\": the first moment vector for the next iteration\n",
        "        \"v\": the second moment fector for the next iteration\n",
        "    }\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE STARTS HERE ################################################\n",
        "\n",
        "    t = t+1\n",
        "    m = beta1*m + (1-beta1)*gradient\n",
        "    v = beta2*v + (1-beta2)*(np.power(gradient, 2))\n",
        "    temp_m = m/(1-np.power(beta1, t))\n",
        "    temp_v = v/(1-np.power(beta2, t))\n",
        "    sq = np.sqrt(temp_v)\n",
        "    weights = weights - ((lr*temp_m)/(sq + epsilon))\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ##################################################\n",
        "    assert isinstance(t, int)\n",
        "    assert isinstance(weights, np.ndarray)\n",
        "    assert weights.shape == gradient.shape \n",
        "    assert isinstance(v, np.ndarray) and v.shape == weights.shape\n",
        "    assert isinstance(m, np.ndarray) and m.shape == weights.shape\n",
        "    return weights, {\"t\": t, \"m\": m, \"v\": v}\n",
        "\n",
        "test_adam(adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4iMSIYRWAmzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33457cfd-a146-4b65-ca05-57ef80e6c3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote solution to adam_hashes.csv\n",
            "Wrote solution to sgd_hashes.csv\n"
          ]
        }
      ],
      "source": [
        "generate_submission_hashes(adam, \"adam_solution_generation_data.pt\", \"adam_hashes.csv\")\n",
        "generate_submission_hashes(sgd_with_momentum, \"sgd_solution_generation_data.pt\", \"sgd_hashes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w_i2aRQxxhVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Optimization_Starter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}